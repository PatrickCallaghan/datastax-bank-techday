package com.kafkaToSparkToDSE

/**
  */
import java.sql.Timestamp
import java.text.{DateFormat, SimpleDateFormat}

import com.datastax.driver.core.Session

import collection.JavaConversions._

import org.apache.spark.sql.SparkSession

import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}



object streaming {


  def main(args: Array[String]) {

    println(s"enterered main")

    val sparkJob = new SparkJob()
    try {
      sparkJob.runJob()
    } catch {
      case ex: Exception =>
        println("error in main running spark job")
    }
  }
}

class SparkJob extends Serializable {

  println(s"before build spark session")

  val sparkSession =
    SparkSession.builder
      .appName("kafka2Spark2DSE")
      .config("spark.cassandra.connection.host", "localhost")
      .getOrCreate()

  println(s"after build spark session")

  def runJob() = {

  val custCol = List("customer_id","address_line1","address_line2","address_type",
		"city","country_code","date_of_birth","email_address","full_name",
		"state_abbreviation","zipcode","zipcode4")

  val cust_raw_df = sparkSession
     .read
     .format("org.apache.spark.sql.cassandra")
     .options(Map( "table" -> "customer", "keyspace" -> "bank"))
     .load()

  
    println(s"after reading cust")
    cust_raw_df.printSchema()

//  this second dataframe allows to select limited number of columns
//   wish could do in one step but can't get it to work
   val cust_df = cust_raw_df.select ( "customer_id","address_line1","address_line2","address_type", "city","country_code","date_of_birth","email_address","full_name",
		"state_abbreviation","zipcode","zipcode4")
    println(s"after select cust")
    cust_df.printSchema()


    println(s"before reading kafka stream after runJob")

    import sparkSession.implicits._
    val lines = sparkSession.readStream
      .format("kafka")
      .option("subscribe", "customer")
      .option("failOnDataLoss", "false")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("startingOffsets", "earliest")
      .load()
      .selectExpr("CAST(value AS STRING)",
                  "CAST(key as STRING)")
      .as[(String, String)] 

    lines.printSchema()
    println(s"finished reading kafka stream ")

    val cols = List("customer_id","address_line1","address_line2","address_type","bill_pay_enrolled","city","country_code","customer_nbr","customer_origin_system","customer_status","customer_type","date_of_birth","email_address","gender","government_id","government_id_type","phone_numbers")
    val df =
      lines.map { line =>
        val payload = line._1.split(";")
        val dob_ts = Timestamp.valueOf(payload(11))
        (payload(0), payload(1),
	 payload(2), payload(3),
	 payload(4), payload(5),
	 payload(6), payload(7),
	 payload(8), payload(9),
	 payload(10), dob_ts,
	 payload(12), payload(13),
	 payload(14), payload(15),
	 payload(16)
         )
      }.toDF(cols: _*)
    println(s"after toDF ")
    df.printSchema()
    println(s"after printschema ")
    println(df.isStreaming)
    println(s"after isStreaming ")
//   join static customer with streaming df
/*   this worked but remaining conditions is just oo ugly
    val joined_df = df.join(cust_df, "customer_id")
*/
    df.createOrReplaceTempView("c_st")
    cust_raw_df.createOrReplaceTempView("c")
    val joined_df = sparkSession.sql ("""  
          select c.customer_id
	  ,count(*) as change_count 
	  from c_st inner join c 
          on c_st.customer_id=c.customer_id
          and c_st.address_line1 <> c.address_line1
          group by c.customer_id
	 """); 
    println(s"after join ")
    joined_df.printSchema()

//  this writes successfully to the console-hurray!
/*
    val query = joined_df.writeStream
      .outputMode("append")
      .queryName("table")
      .format("console")
      .start()
*/

/*
    val query = joined_df.writeStream
      .option("checkpointLocation", "/tmp")
      .format("org.apache.spark.sql.cassandra")
      .option("keyspace", "dsbank")
      .option("table", "account_fraud")
      .outputMode(OutputMode.Update)
      .start()
    println (s"after write to  account_fraud")

    // Group the data by window and word and compute the count of each group

    query.awaitTermination()
*/
      sparkSession.stop()
  }
}
